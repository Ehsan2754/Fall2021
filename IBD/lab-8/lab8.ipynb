{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b65daf1f",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee4684a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2931336\r\n",
      "drwxrwxrwx 1 root   root        4096 Dec  1 13:47 \u001b[0m\u001b[34;42m.\u001b[0m/\r\n",
      "drwxr-xr-x 1 root   root        4096 Nov 20 11:30 \u001b[01;34m..\u001b[0m/\r\n",
      "-rw------- 1 jovyan users        111 Nov 30 07:44 .bash_history\r\n",
      "drwxr-xr-x 1 jovyan users       4096 Nov 24 11:00 \u001b[01;34m.cache\u001b[0m/\r\n",
      "drwxr-xr-x 1 jovyan users       4096 Nov 24 11:04 \u001b[01;34m.ipynb_checkpoints\u001b[0m/\r\n",
      "drwxr-xr-x 1 jovyan users       4096 Nov 24 11:04 \u001b[01;34m.ipython\u001b[0m/\r\n",
      "drwxr-xr-x 1 jovyan users       4096 Nov 30 07:40 \u001b[01;34m.jupyter\u001b[0m/\r\n",
      "-rw-r--r-- 1 jovyan users      21107 Dec  1 13:47 lab8.ipynb\r\n",
      "drwxr-xr-x 1 jovyan users       4096 Nov 24 10:59 \u001b[01;34m.local\u001b[0m/\r\n",
      "drwxr-xr-x 1 jovyan users       4096 Nov 24 11:35 \u001b[01;34mspark-warehouse\u001b[0m/\r\n",
      "-rwxrwxrwx 1 root   root  3001659271 Dec 19  2011 \u001b[01;32mtrain_triplets.txt\u001b[0m*\r\n",
      "drwxrwxrwx 1 root   root        4096 Nov 24 10:20 \u001b[34;42myelp-dataset\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "# !pip install pyspark\n",
    "%ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4f27d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "839e18cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/12/01 13:48:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# since everyone will be using cluster at the same time\n",
    "# let's make sure that everyone has resource. that is why \n",
    "# the configuration uses dynamic resource allocation and\n",
    "# maximum 1 executor \n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\")\\\n",
    "    .config(\"spark.dynamicAllocation.shuffleTracking.enabled\", \"true\")\\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"1\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "350324b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "songs_df = spark.read.load(\"./train_triplets.txt\",\n",
    "                     format=\"csv\", sep=\"\\t\", inferSchema=\"true\", \n",
    "                     header=\"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "621bf494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "songs_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6ab5c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_df = songs_df.withColumnRenamed(\"_c0\", \"user\")\\\n",
    "                   .withColumnRenamed(\"_c1\", \"song\")\\\n",
    "                   .withColumnRenamed(\"_c2\", \"play_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da05428f",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_df.createOrReplaceTempView(\"songs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42d36dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "played_more_than_10_times = spark.sql(\"select song from songs where play_count > 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ff13939",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2043582"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "played_more_than_10_times.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98d80b7",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9cebb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/01 13:49:23 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "business = spark.read.json(\"./yelp-dataset/yelp_academic_dataset_business.json\")\n",
    "reviews = spark.read.json(\"./yelp-dataset/yelp_academic_dataset_review.json\")\n",
    "users = spark.read.json(\"./yelp-dataset/yelp_academic_dataset_user.json\")\n",
    "business.createOrReplaceTempView(\"business\")\n",
    "reviews.createOrReplaceTempView(\"reviews\")\n",
    "users.createOrReplaceTempView(\"users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e115128f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:======================>                                    (3 + 5) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|state|count|\n",
      "+-----+-----+\n",
      "|   AZ|56686|\n",
      "|   NV|36312|\n",
      "|   ON|33412|\n",
      "|   NC|14720|\n",
      "|   OH|14697|\n",
      "|   PA|11216|\n",
      "|   QC| 9219|\n",
      "|   AB| 8012|\n",
      "|   WI| 5154|\n",
      "|   IL| 1932|\n",
      "|   SC| 1162|\n",
      "|   NY|   22|\n",
      "|   CA|   19|\n",
      "|   TX|    6|\n",
      "|   FL|    4|\n",
      "|  XGM|    4|\n",
      "|   AL|    3|\n",
      "|   WA|    3|\n",
      "|   CT|    3|\n",
      "|   VA|    2|\n",
      "+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Query 1\n",
    "spark.sql(\"select state, count(state) as count from business group by state order by count(state) desc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f004c593",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:==============>                                           (2 + 6) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|count(DISTINCT category)|\n",
      "+------------------------+\n",
      "|                    2468|\n",
      "+------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Query 2\n",
    "spark.sql(\"\"\"\n",
    "select count(distinct(*)) from (\n",
    "    select explode(split(categories, \\\",\\s*\\\")) as category from business\n",
    ")\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "647d0792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 17:>                                                         (0 + 8) / 8]\r",
      "\r",
      "[Stage 17:=======>                                                  (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------------+\n",
      "|         category|count(category)|\n",
      "+-----------------+---------------+\n",
      "|      Restaurants|           2815|\n",
      "|         Shopping|           2416|\n",
      "|    Home Services|           2302|\n",
      "|             Food|           1672|\n",
      "| Health & Medical|           1577|\n",
      "|   Local Services|           1444|\n",
      "|      Restaurants|           1184|\n",
      "|       Automotive|           1164|\n",
      "|    Beauty & Spas|           1115|\n",
      "|    Home Services|            843|\n",
      "+-----------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Query 3\n",
    "spark.sql(\"\"\"\n",
    "select category, count(category) from \n",
    "    (\n",
    "        select explode(split(categories, \\\",\\s*\\\")) as category \n",
    "        from business where city=\\\"Phoenix\\\"\n",
    "    )\n",
    "group by category order by count(category) desc limit 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "524eafe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:==================================================>     (17 + 2) / 19]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|friend_count|\n",
      "+------------+\n",
      "|        4166|\n",
      "+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 20:=====================================================>  (18 + 1) / 19]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Query 4 \n",
    "spark.sql(\"\"\"\n",
    "select \n",
    "    count(*) as friend_count \n",
    "from \n",
    "    users \n",
    "where \n",
    "    size(split(friends, \\\",\\s*\\\")) > 1000\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6bf4910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 6\n",
    "# spark.sql(\n",
    "# \"\"\"with p1 as \n",
    "#     (SELECT business_id, user_id, MAX(to_date(date))\n",
    "#         FROM reviews GROUP BY user_id, business_id),\n",
    "#     p2 as (SELECT business_id FROM business WHERE categories LIKE ('%Chinese%') \n",
    "#         AND categories LIKE ('%Restaurants%')\n",
    "#     ),\n",
    "#     p3 as (SELECT p1.user_id \n",
    "#         from p1 inner \n",
    "#         join p2 \n",
    "#         on p2.business_id = p1.business_id)\n",
    "#     SELECT COUNT(*) FROM users inner join p3 on p3.user_id = users.user_id\n",
    "# \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d8b66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:>                                                      (0 + 8) / 3800]\r"
     ]
    }
   ],
   "source": [
    "#Query 6\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "with dates as (select max(to_date(date)) as date, user_id\n",
    "from reviews\n",
    "group by user_id)\n",
    "\n",
    "select COUNT(f.*)\n",
    "from reviews as r1\n",
    "join dates as d\n",
    "on d.user_id = r1.user_id\n",
    "\n",
    "join business as b\n",
    "on b.business_id = r1.business_id\n",
    "\n",
    "join users as f\n",
    "on array_contains(split(f.friends, \\\",\\s*\\\"), r1.user_id)\n",
    "\n",
    "where \n",
    "    date_trunc(\"DAY\", r1.date) = date_trunc(\"DAY\", d.date)\n",
    "    and\n",
    "    array_contains(split(b.categories, \\\",\\s*\\\"), 'Chinese')\n",
    "\n",
    "\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
